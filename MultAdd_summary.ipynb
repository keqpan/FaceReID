{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summary.model_summary import model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_madd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b5fb2db1c755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_madd'"
     ]
    }
   ],
   "source": [
    "from summary import model_madd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.denoise.pydl import ResNet_Den\n",
    "\n",
    "from MMNet_TBPTT import *\n",
    "from problems import *\n",
    "\n",
    "from networks.faceid.mobile import MobileFacenet\n",
    "from networks.faceid.mobile import ArcMarginProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.denoise.pydl import ResNet_Den\n",
    "from MMNet_TBPTT import *\n",
    "from problems import *\n",
    "denoiser = ResNet_Den(5, weightnorm=True)\n",
    "\n",
    "max_iter = 5\n",
    "mmnet = MMNet(denoiser, max_iter=max_iter)\n",
    "\n",
    "faceid = MobileFacenet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "PReLU is unsupported!\n",
      "          module name  input shape output shape  parameter quantity inference memory(MB)        MAdd duration percent\n",
      "0          conv1.conv    3 112  96   64  56  48                1728               0.66MB   9,117,696            2.97%\n",
      "1            conv1.bn   64  56  48   64  56  48                 128               0.66MB     688,128            0.49%\n",
      "2         conv1.prelu   64  56  48   64  56  48                  64               0.66MB           0            1.24%\n",
      "3       dw_conv1.conv   64  56  48   64  56  48                 576               0.66MB   2,924,544            1.84%\n",
      "4         dw_conv1.bn   64  56  48   64  56  48                 128               0.66MB     688,128            0.43%\n",
      "5      dw_conv1.prelu   64  56  48   64  56  48                  64               0.66MB           0            1.29%\n",
      "6     blocks.0.conv.0   64  56  48  128  56  48                8192               1.31MB  43,696,128            2.00%\n",
      "7     blocks.0.conv.1  128  56  48  128  56  48                 256               1.31MB   1,376,256            0.98%\n",
      "8     blocks.0.conv.2  128  56  48  128  56  48                 128               1.31MB           0            1.77%\n",
      "9     blocks.0.conv.3  128  56  48  128  28  24                1152               0.33MB   1,462,272            1.53%\n",
      "10    blocks.0.conv.4  128  28  24  128  28  24                 256               0.33MB     344,064            0.29%\n",
      "11    blocks.0.conv.5  128  28  24  128  28  24                 128               0.33MB           0            0.76%\n",
      "12    blocks.0.conv.6  128  28  24   64  28  24                8192               0.16MB  10,967,040            1.34%\n",
      "13    blocks.0.conv.7   64  28  24   64  28  24                 128               0.16MB     172,032            0.23%\n",
      "14    blocks.1.conv.0   64  28  24  128  28  24                8192               0.33MB  10,924,032            1.35%\n",
      "15    blocks.1.conv.1  128  28  24  128  28  24                 256               0.33MB     344,064            0.29%\n",
      "16    blocks.1.conv.2  128  28  24  128  28  24                 128               0.33MB           0            0.70%\n",
      "17    blocks.1.conv.3  128  28  24  128  28  24                1152               0.33MB   1,462,272            1.06%\n",
      "18    blocks.1.conv.4  128  28  24  128  28  24                 256               0.33MB     344,064            0.30%\n",
      "19    blocks.1.conv.5  128  28  24  128  28  24                 128               0.33MB           0            0.77%\n",
      "20    blocks.1.conv.6  128  28  24   64  28  24                8192               0.16MB  10,967,040            1.30%\n",
      "21    blocks.1.conv.7   64  28  24   64  28  24                 128               0.16MB     172,032            0.24%\n",
      "22    blocks.2.conv.0   64  28  24  128  28  24                8192               0.33MB  10,924,032            1.34%\n",
      "23    blocks.2.conv.1  128  28  24  128  28  24                 256               0.33MB     344,064            0.29%\n",
      "24    blocks.2.conv.2  128  28  24  128  28  24                 128               0.33MB           0            0.69%\n",
      "25    blocks.2.conv.3  128  28  24  128  28  24                1152               0.33MB   1,462,272            0.94%\n",
      "26    blocks.2.conv.4  128  28  24  128  28  24                 256               0.33MB     344,064            0.27%\n",
      "27    blocks.2.conv.5  128  28  24  128  28  24                 128               0.33MB           0            0.76%\n",
      "28    blocks.2.conv.6  128  28  24   64  28  24                8192               0.16MB  10,967,040            1.27%\n",
      "29    blocks.2.conv.7   64  28  24   64  28  24                 128               0.16MB     172,032            0.22%\n",
      "30    blocks.3.conv.0   64  28  24  128  28  24                8192               0.33MB  10,924,032            1.34%\n",
      "31    blocks.3.conv.1  128  28  24  128  28  24                 256               0.33MB     344,064            0.28%\n",
      "32    blocks.3.conv.2  128  28  24  128  28  24                 128               0.33MB           0            0.70%\n",
      "33    blocks.3.conv.3  128  28  24  128  28  24                1152               0.33MB   1,462,272            0.94%\n",
      "34    blocks.3.conv.4  128  28  24  128  28  24                 256               0.33MB     344,064            0.28%\n",
      "35    blocks.3.conv.5  128  28  24  128  28  24                 128               0.33MB           0            0.76%\n",
      "36    blocks.3.conv.6  128  28  24   64  28  24                8192               0.16MB  10,967,040            1.29%\n",
      "37    blocks.3.conv.7   64  28  24   64  28  24                 128               0.16MB     172,032            0.22%\n",
      "38    blocks.4.conv.0   64  28  24  128  28  24                8192               0.33MB  10,924,032            1.31%\n",
      "39    blocks.4.conv.1  128  28  24  128  28  24                 256               0.33MB     344,064            0.28%\n",
      "40    blocks.4.conv.2  128  28  24  128  28  24                 128               0.33MB           0            0.70%\n",
      "41    blocks.4.conv.3  128  28  24  128  28  24                1152               0.33MB   1,462,272            0.93%\n",
      "42    blocks.4.conv.4  128  28  24  128  28  24                 256               0.33MB     344,064            0.27%\n",
      "43    blocks.4.conv.5  128  28  24  128  28  24                 128               0.33MB           0            0.75%\n",
      "44    blocks.4.conv.6  128  28  24   64  28  24                8192               0.16MB  10,967,040            1.27%\n",
      "45    blocks.4.conv.7   64  28  24   64  28  24                 128               0.16MB     172,032            0.22%\n",
      "46    blocks.5.conv.0   64  28  24  256  28  24               16384               0.66MB  21,848,064            1.36%\n",
      "47    blocks.5.conv.1  256  28  24  256  28  24                 512               0.66MB     688,128            0.39%\n",
      "48    blocks.5.conv.2  256  28  24  256  28  24                 256               0.66MB           0            1.00%\n",
      "49    blocks.5.conv.3  256  28  24  256  14  12                2304               0.16MB     731,136            0.88%\n",
      "50    blocks.5.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.30%\n",
      "51    blocks.5.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.60%\n",
      "52    blocks.5.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.26%\n",
      "53    blocks.5.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.23%\n",
      "54    blocks.6.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            2.10%\n",
      "55    blocks.6.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.29%\n",
      "56    blocks.6.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.56%\n",
      "57    blocks.6.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.79%\n",
      "58    blocks.6.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.29%\n",
      "59    blocks.6.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.62%\n",
      "60    blocks.6.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.25%\n",
      "61    blocks.6.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.22%\n",
      "62    blocks.7.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            1.21%\n",
      "63    blocks.7.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.28%\n",
      "64    blocks.7.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.57%\n",
      "65    blocks.7.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.79%\n",
      "66    blocks.7.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.28%\n",
      "67    blocks.7.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.62%\n",
      "68    blocks.7.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.25%\n",
      "69    blocks.7.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.22%\n",
      "70    blocks.8.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            1.21%\n",
      "71    blocks.8.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.29%\n",
      "72    blocks.8.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.56%\n",
      "73    blocks.8.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.80%\n",
      "74    blocks.8.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.28%\n",
      "75    blocks.8.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.63%\n",
      "76    blocks.8.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.25%\n",
      "77    blocks.8.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.24%\n",
      "78    blocks.9.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            1.21%\n",
      "79    blocks.9.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.27%\n",
      "80    blocks.9.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.65%\n",
      "81    blocks.9.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.87%\n",
      "82    blocks.9.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.29%\n",
      "83    blocks.9.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.58%\n",
      "84    blocks.9.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.24%\n",
      "85    blocks.9.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.23%\n",
      "86   blocks.10.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            1.21%\n",
      "87   blocks.10.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.27%\n",
      "88   blocks.10.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.65%\n",
      "89   blocks.10.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.85%\n",
      "90   blocks.10.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.29%\n",
      "91   blocks.10.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.59%\n",
      "92   blocks.10.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.26%\n",
      "93   blocks.10.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.23%\n",
      "94   blocks.11.conv.0  128  14  12  256  14  12               32768               0.16MB  10,967,040            1.22%\n",
      "95   blocks.11.conv.1  256  14  12  256  14  12                 512               0.16MB     172,032            0.28%\n",
      "96   blocks.11.conv.2  256  14  12  256  14  12                 256               0.16MB           0            0.64%\n",
      "97   blocks.11.conv.3  256  14  12  256  14  12                2304               0.16MB     731,136            0.85%\n",
      "98   blocks.11.conv.4  256  14  12  256  14  12                 512               0.16MB     172,032            0.28%\n",
      "99   blocks.11.conv.5  256  14  12  256  14  12                 256               0.16MB           0            0.61%\n",
      "100  blocks.11.conv.6  256  14  12  128  14  12               32768               0.08MB  10,988,544            1.27%\n",
      "101  blocks.11.conv.7  128  14  12  128  14  12                 256               0.08MB      86,016            0.24%\n",
      "102  blocks.12.conv.0  128  14  12  512  14  12               65536               0.33MB  21,934,080            1.32%\n",
      "103  blocks.12.conv.1  512  14  12  512  14  12                1024               0.33MB     344,064            0.44%\n",
      "104  blocks.12.conv.2  512  14  12  512  14  12                 512               0.33MB           0            0.79%\n",
      "105  blocks.12.conv.3  512  14  12  512   7   6                4608               0.08MB     365,568            0.82%\n",
      "106  blocks.12.conv.4  512   7   6  512   7   6                1024               0.08MB      86,016            0.39%\n",
      "107  blocks.12.conv.5  512   7   6  512   7   6                 512               0.08MB           0            0.52%\n",
      "108  blocks.12.conv.6  512   7   6  128   7   6               65536               0.02MB   5,499,648            1.27%\n",
      "109  blocks.12.conv.7  128   7   6  128   7   6                 256               0.02MB      21,504            0.22%\n",
      "110  blocks.13.conv.0  128   7   6  256   7   6               32768               0.04MB   2,741,760            1.17%\n",
      "111  blocks.13.conv.1  256   7   6  256   7   6                 512               0.04MB      43,008            0.27%\n",
      "112  blocks.13.conv.2  256   7   6  256   7   6                 256               0.04MB           0            0.45%\n",
      "113  blocks.13.conv.3  256   7   6  256   7   6                2304               0.04MB     182,784            0.81%\n",
      "114  blocks.13.conv.4  256   7   6  256   7   6                 512               0.04MB      43,008            0.27%\n",
      "115  blocks.13.conv.5  256   7   6  256   7   6                 256               0.04MB           0            0.46%\n",
      "116  blocks.13.conv.6  256   7   6  128   7   6               32768               0.02MB   2,747,136            1.23%\n",
      "117  blocks.13.conv.7  128   7   6  128   7   6                 256               0.02MB      21,504            0.21%\n",
      "118  blocks.14.conv.0  128   7   6  256   7   6               32768               0.04MB   2,741,760            1.18%\n",
      "119  blocks.14.conv.1  256   7   6  256   7   6                 512               0.04MB      43,008            0.25%\n",
      "120  blocks.14.conv.2  256   7   6  256   7   6                 256               0.04MB           0            0.45%\n",
      "121  blocks.14.conv.3  256   7   6  256   7   6                2304               0.04MB     182,784            0.81%\n",
      "122  blocks.14.conv.4  256   7   6  256   7   6                 512               0.04MB      43,008            0.26%\n",
      "123  blocks.14.conv.5  256   7   6  256   7   6                 256               0.04MB           0            1.32%\n",
      "124  blocks.14.conv.6  256   7   6  128   7   6               32768               0.02MB   2,747,136            1.26%\n",
      "125  blocks.14.conv.7  128   7   6  128   7   6                 256               0.02MB      21,504            0.21%\n",
      "126        conv2.conv  128   7   6  512   7   6               65536               0.08MB   5,483,520            1.21%\n",
      "127          conv2.bn  512   7   6  512   7   6                1024               0.08MB      86,016            0.37%\n",
      "128       conv2.prelu  512   7   6  512   7   6                 512               0.08MB           0            0.47%\n",
      "129      linear7.conv  512   7   6  512   1   1               21504               0.00MB      42,496            0.83%\n",
      "130        linear7.bn  512   1   1  512   1   1                1024               0.00MB       2,048            0.34%\n",
      "131      linear1.conv  512   1   1  128   1   1               65536               0.00MB     130,944            1.07%\n",
      "132        linear1.bn  128   1   1  128   1   1                 256               0.00MB         512            0.20%\n",
      "=====================================================================================================================\n",
      "total parameters quantity: 999,552\n",
      "total memory: 29.99MB\n",
      "total MAdd: 387,059,840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = faceid\n",
    "input_size = (3, 112, 96)\n",
    "summary = model_summary(model, input_size, query_granularity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "387M MAdd\n",
    "1M parameters\n",
    "inference memory(MB) 29.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               module name   input shape  output shape  parameter quantity inference memory(MB)         MAdd duration percent\n",
      "0                    conv1     3 112  96    64 112  96                1728               2.62MB   36,470,784            2.72%\n",
      "1                      bn1    64 112  96    64 112  96                 128               2.62MB    2,752,512            1.62%\n",
      "2                     relu    64 112  96    64 112  96                   0               2.62MB      688,128            0.15%\n",
      "3           layer1.0.conv1    64 112  96    64 112  96                4096               2.62MB   87,392,256            3.71%\n",
      "4             layer1.0.bn1    64 112  96    64 112  96                 128               2.62MB    2,752,512            1.54%\n",
      "5           layer1.0.conv2    64 112  96    64  56  48               36864               0.66MB  198,008,832            2.71%\n",
      "6             layer1.0.bn2    64  56  48    64  56  48                 128               0.66MB      688,128            0.36%\n",
      "7           layer1.0.conv3    64  56  48   256  56  48               16384               2.62MB   87,392,256            2.31%\n",
      "8             layer1.0.bn3   256  56  48   256  56  48                 512               2.62MB    2,752,512            1.31%\n",
      "9            layer1.0.relu   256  56  48   256  56  48                   0               2.62MB      688,128            0.10%\n",
      "10   layer1.0.downsample.0    64 112  96   256  56  48               16384               2.62MB   87,392,256            2.86%\n",
      "11   layer1.0.downsample.1   256  56  48   256  56  48                 512               2.62MB    2,752,512            0.93%\n",
      "12          layer1.1.conv1   256  56  48    64  56  48               16384               0.66MB   87,908,352            1.45%\n",
      "13            layer1.1.bn1    64  56  48    64  56  48                 128               0.66MB      688,128            0.23%\n",
      "14          layer1.1.conv2    64  56  48    64  56  48               36864               0.66MB  198,008,832            1.48%\n",
      "15            layer1.1.bn2    64  56  48    64  56  48                 128               0.66MB      688,128            0.22%\n",
      "16          layer1.1.conv3    64  56  48   256  56  48               16384               2.62MB   87,392,256            1.66%\n",
      "17            layer1.1.bn3   256  56  48   256  56  48                 512               2.62MB    2,752,512            0.71%\n",
      "18           layer1.1.relu   256  56  48   256  56  48                   0               2.62MB      688,128            0.07%\n",
      "19          layer1.2.conv1   256  56  48    64  56  48               16384               0.66MB   87,908,352            1.34%\n",
      "20            layer1.2.bn1    64  56  48    64  56  48                 128               0.66MB      688,128            0.17%\n",
      "21          layer1.2.conv2    64  56  48    64  56  48               36864               0.66MB  198,008,832            1.22%\n",
      "22            layer1.2.bn2    64  56  48    64  56  48                 128               0.66MB      688,128            0.16%\n",
      "23          layer1.2.conv3    64  56  48   256  56  48               16384               2.62MB   87,392,256            1.40%\n",
      "24            layer1.2.bn3   256  56  48   256  56  48                 512               2.62MB    2,752,512            0.51%\n",
      "25           layer1.2.relu   256  56  48   256  56  48                   0               2.62MB      688,128            0.07%\n",
      "26          layer2.0.conv1   256  56  48   128  56  48               32768               1.31MB  175,816,704            1.62%\n",
      "27            layer2.0.bn1   128  56  48   128  56  48                 256               1.31MB    1,376,256            0.31%\n",
      "28          layer2.0.conv2   128  56  48   128  28  24              147456               0.33MB  198,094,848            1.17%\n",
      "29            layer2.0.bn2   128  28  24   128  28  24                 256               0.33MB      344,064            0.13%\n",
      "30          layer2.0.conv3   128  28  24   512  28  24               65536               1.31MB   87,736,320            0.97%\n",
      "31            layer2.0.bn3   512  28  24   512  28  24                1024               1.31MB    1,376,256            0.37%\n",
      "32           layer2.0.relu   512  28  24   512  28  24                   0               1.31MB      344,064            0.05%\n",
      "33   layer2.0.downsample.0   256  56  48   512  28  24              131072               1.31MB  175,816,704            1.75%\n",
      "34   layer2.0.downsample.1   512  28  24   512  28  24                1024               1.31MB    1,376,256            0.38%\n",
      "35          layer2.1.conv1   512  28  24   128  28  24               65536               0.33MB   87,994,368            0.98%\n",
      "36            layer2.1.bn1   128  28  24   128  28  24                 256               0.33MB      344,064            0.13%\n",
      "37          layer2.1.conv2   128  28  24   128  28  24              147456               0.33MB  198,094,848            0.99%\n",
      "38            layer2.1.bn2   128  28  24   128  28  24                 256               0.33MB      344,064            0.12%\n",
      "39          layer2.1.conv3   128  28  24   512  28  24               65536               1.31MB   87,736,320            0.93%\n",
      "40            layer2.1.bn3   512  28  24   512  28  24                1024               1.31MB    1,376,256            0.36%\n",
      "41           layer2.1.relu   512  28  24   512  28  24                   0               1.31MB      344,064            0.04%\n",
      "42          layer2.2.conv1   512  28  24   128  28  24               65536               0.33MB   87,994,368            0.94%\n",
      "43            layer2.2.bn1   128  28  24   128  28  24                 256               0.33MB      344,064            0.12%\n",
      "44          layer2.2.conv2   128  28  24   128  28  24              147456               0.33MB  198,094,848            0.95%\n",
      "45            layer2.2.bn2   128  28  24   128  28  24                 256               0.33MB      344,064            0.12%\n",
      "46          layer2.2.conv3   128  28  24   512  28  24               65536               1.31MB   87,736,320            0.93%\n",
      "47            layer2.2.bn3   512  28  24   512  28  24                1024               1.31MB    1,376,256            0.36%\n",
      "48           layer2.2.relu   512  28  24   512  28  24                   0               1.31MB      344,064            0.03%\n",
      "49          layer2.3.conv1   512  28  24   128  28  24               65536               0.33MB   87,994,368            0.99%\n",
      "50            layer2.3.bn1   128  28  24   128  28  24                 256               0.33MB      344,064            0.12%\n",
      "51          layer2.3.conv2   128  28  24   128  28  24              147456               0.33MB  198,094,848            0.94%\n",
      "52            layer2.3.bn2   128  28  24   128  28  24                 256               0.33MB      344,064            0.12%\n",
      "53          layer2.3.conv3   128  28  24   512  28  24               65536               1.31MB   87,736,320            1.02%\n",
      "54            layer2.3.bn3   512  28  24   512  28  24                1024               1.31MB    1,376,256            0.36%\n",
      "55           layer2.3.relu   512  28  24   512  28  24                   0               1.31MB      344,064            0.04%\n",
      "56          layer3.0.conv1   512  28  24   256  28  24              131072               0.66MB  175,988,736            1.31%\n",
      "57            layer3.0.bn1   256  28  24   256  28  24                 512               0.66MB      688,128            0.20%\n",
      "58          layer3.0.conv2   256  28  24   256  14  12              589824               0.16MB  198,137,856            1.13%\n",
      "59            layer3.0.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.14%\n",
      "60          layer3.0.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.81%\n",
      "61            layer3.0.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.43%\n",
      "62           layer3.0.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.03%\n",
      "63   layer3.0.downsample.0   512  28  24  1024  14  12              524288               0.66MB  175,988,736            1.59%\n",
      "64   layer3.0.downsample.1  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.44%\n",
      "65          layer3.1.conv1  1024  14  12   256  14  12              262144               0.16MB   88,037,376            0.90%\n",
      "66            layer3.1.bn1   256  14  12   256  14  12                 512               0.16MB      172,032            0.14%\n",
      "67          layer3.1.conv2   256  14  12   256  14  12              589824               0.16MB  198,137,856            0.97%\n",
      "68            layer3.1.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "69          layer3.1.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.81%\n",
      "70            layer3.1.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.40%\n",
      "71           layer3.1.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.02%\n",
      "72          layer3.2.conv1  1024  14  12   256  14  12              262144               0.16MB   88,037,376            0.85%\n",
      "73            layer3.2.bn1   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "74          layer3.2.conv2   256  14  12   256  14  12              589824               0.16MB  198,137,856            0.98%\n",
      "75            layer3.2.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "76          layer3.2.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.82%\n",
      "77            layer3.2.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.43%\n",
      "78           layer3.2.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.02%\n",
      "79          layer3.3.conv1  1024  14  12   256  14  12              262144               0.16MB   88,037,376            0.85%\n",
      "80            layer3.3.bn1   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "81          layer3.3.conv2   256  14  12   256  14  12              589824               0.16MB  198,137,856            0.99%\n",
      "82            layer3.3.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.14%\n",
      "83          layer3.3.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.79%\n",
      "84            layer3.3.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.42%\n",
      "85           layer3.3.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.02%\n",
      "86          layer3.4.conv1  1024  14  12   256  14  12              262144               0.16MB   88,037,376            0.84%\n",
      "87            layer3.4.bn1   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "88          layer3.4.conv2   256  14  12   256  14  12              589824               0.16MB  198,137,856            1.01%\n",
      "89            layer3.4.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "90          layer3.4.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.80%\n",
      "91            layer3.4.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.40%\n",
      "92           layer3.4.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.02%\n",
      "93          layer3.5.conv1  1024  14  12   256  14  12              262144               0.16MB   88,037,376            0.90%\n",
      "94            layer3.5.bn1   256  14  12   256  14  12                 512               0.16MB      172,032            0.13%\n",
      "95          layer3.5.conv2   256  14  12   256  14  12              589824               0.16MB  198,137,856            1.00%\n",
      "96            layer3.5.bn2   256  14  12   256  14  12                 512               0.16MB      172,032            0.14%\n",
      "97          layer3.5.conv3   256  14  12  1024  14  12              262144               0.66MB   87,908,352            0.82%\n",
      "98            layer3.5.bn3  1024  14  12  1024  14  12                2048               0.66MB      688,128            0.41%\n",
      "99           layer3.5.relu  1024  14  12  1024  14  12                   0               0.66MB      172,032            0.02%\n",
      "100         layer4.0.conv1  1024  14  12   512  14  12              524288               0.33MB  176,074,752            1.25%\n",
      "101           layer4.0.bn1   512  14  12   512  14  12                1024               0.33MB      344,064            0.23%\n",
      "102         layer4.0.conv2   512  14  12   512   7   6             2359296               0.08MB  198,159,360            2.40%\n",
      "103           layer4.0.bn2   512   7   6   512   7   6                1024               0.08MB       86,016            0.22%\n",
      "104         layer4.0.conv3   512   7   6  2048   7   6             1048576               0.33MB   87,994,368            1.43%\n",
      "105           layer4.0.bn3  2048   7   6  2048   7   6                4096               0.33MB      344,064            0.66%\n",
      "106          layer4.0.relu  2048   7   6  2048   7   6                   0               0.33MB       86,016            0.04%\n",
      "107  layer4.0.downsample.0  1024  14  12  2048   7   6             2097152               0.33MB  176,074,752            2.16%\n",
      "108  layer4.0.downsample.1  2048   7   6  2048   7   6                4096               0.33MB      344,064            0.73%\n",
      "109         layer4.1.conv1  2048   7   6   512   7   6             1048576               0.08MB   88,058,880            1.28%\n",
      "110           layer4.1.bn1   512   7   6   512   7   6                1024               0.08MB       86,016            0.22%\n",
      "111         layer4.1.conv2   512   7   6   512   7   6             2359296               0.08MB  198,159,360            2.46%\n",
      "112           layer4.1.bn2   512   7   6   512   7   6                1024               0.08MB       86,016            0.23%\n",
      "113         layer4.1.conv3   512   7   6  2048   7   6             1048576               0.33MB   87,994,368            1.36%\n",
      "114           layer4.1.bn3  2048   7   6  2048   7   6                4096               0.33MB      344,064            0.69%\n",
      "115          layer4.1.relu  2048   7   6  2048   7   6                   0               0.33MB       86,016            0.03%\n",
      "116         layer4.2.conv1  2048   7   6   512   7   6             1048576               0.08MB   88,058,880            1.23%\n",
      "117           layer4.2.bn1   512   7   6   512   7   6                1024               0.08MB       86,016            0.21%\n",
      "118         layer4.2.conv2   512   7   6   512   7   6             2359296               0.08MB  198,159,360            2.43%\n",
      "119           layer4.2.bn2   512   7   6   512   7   6                1024               0.08MB       86,016            0.21%\n",
      "120         layer4.2.conv3   512   7   6  2048   7   6             1048576               0.33MB   87,994,368            1.36%\n",
      "121           layer4.2.bn3  2048   7   6  2048   7   6                4096               0.33MB      344,064            0.67%\n",
      "122          layer4.2.relu  2048   7   6  2048   7   6                   0               0.33MB       86,016            0.03%\n",
      "123                    fc5         86016           512            44040704               0.00MB   88,079,872            7.19%\n",
      "=============================================================================================================================\n",
      "total parameters quantity: 67,541,056\n",
      "total memory: 97.29MB\n",
      "total MAdd: 7,031,054,848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from networks.faceid.resnet import resnet_face18, resnet50\n",
    "# faceid = resnet_face18(use_se = False)\n",
    "faceid = resnet50()\n",
    "\n",
    "model = faceid\n",
    "input_size = (3, 112, 96)\n",
    "summary = model_summary(model, input_size, query_granularity=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total parameters quantity: 19,8 M\n",
    "total memory: 26.34MB\n",
    "MAdd: 2976.5 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 5 required positional arguments: 'xpre', 'mosaic', 'M', 'noise_sigma', and 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ae8ffb2e9eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_granularity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/safin/FaceReID/summary/model_summary.py\u001b[0m in \u001b[0;36mmodel_summary\u001b[0;34m(model, input_size, query_granularity)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmodel_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCModelHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mleaf_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_leaf_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0msummary_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_leaf_modules_to_summary_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/safin/FaceReID/summary/model_hook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, input_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add module duration time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/safin/miniconda3/envs/py36torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 5 required positional arguments: 'xpre', 'mosaic', 'M', 'noise_sigma', and 'k'"
     ]
    }
   ],
   "source": [
    "model = mmnet\n",
    "input_size = (3, 112, 96)\n",
    "summary = model_summary(model, input_size, query_granularity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_summary import model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser.conv1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = ResNet_Den(5, weightnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2Proj is unsupported!\n",
      "       module name  input shape output shape  parameter quantity inference memory(MB)         MAdd duration percent\n",
      "0            conv1    3 116 100   64 112  96                4928               2.62MB  103,219,200            3.01%\n",
      "1   layer1.0.conv1   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.17%\n",
      "2   layer1.0.relu1   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.85%\n",
      "3   layer1.0.relu2   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.94%\n",
      "4   layer1.0.conv2   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.07%\n",
      "5   layer1.1.conv1   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.11%\n",
      "6   layer1.1.relu1   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.79%\n",
      "7   layer1.1.relu2   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.70%\n",
      "8   layer1.1.conv2   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.04%\n",
      "9   layer1.2.conv1   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.15%\n",
      "10  layer1.2.relu1   64 112  96   64 112  96                  64               2.62MB    2,752,512            4.15%\n",
      "11  layer1.2.relu2   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.62%\n",
      "12  layer1.2.conv2   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.11%\n",
      "13  layer1.3.conv1   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.10%\n",
      "14  layer1.3.relu1   64 112  96   64 112  96                  64               2.62MB    2,752,512            4.33%\n",
      "15  layer1.3.relu2   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.61%\n",
      "16  layer1.3.conv2   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.14%\n",
      "17  layer1.4.conv1   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.18%\n",
      "18  layer1.4.relu1   64 112  96   64 112  96                  64               2.62MB    2,752,512            4.36%\n",
      "19  layer1.4.relu2   64 112  96   64 112  96                  64               2.62MB    2,752,512            2.34%\n",
      "20  layer1.4.conv2   64 114  98   64 112  96               36992               2.62MB  792,723,456            6.11%\n",
      "21        conv_out   64 112  96    3 112  96                4867               0.12MB  103,219,200            3.36%\n",
      "22          l2proj    3 112  96    3 112  96                   0               0.12MB            0            0.76%\n",
      "===================================================================================================================\n",
      "total parameters quantity: 380,355\n",
      "total memory: 55.37MB\n",
      "total MAdd: 8,161,198,080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = denoiser\n",
    "input_size = (3, 112, 96)\n",
    "summary = model_summary(model, input_size, query_granularity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/safin/\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "from pydl.networks.UDNet.net import UDNet\n",
    "from pydl.networks.ResDNet.net import ResDNet\n",
    "\n",
    "from pydl.nnLayers import modules\n",
    "from pydl.nnLayers import init\n",
    "from pydl.utils import loadmat\n",
    "from pydl.nnLayers.cascades import nconv2D, nconv_transpose2D\n",
    "from pydl.utils import formatInput2Tuple, getPad2RetainShape\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "    \n",
    "class L2Proj(nn.Module):\n",
    "    # L2Prox layer\n",
    "    # Y = NN_L2TRPROX(X,EPSILON) computes the Proximal map layer for the\n",
    "    #   indicator function :\n",
    "    #\n",
    "    #                      { 0 if ||X|| <= EPSILON\n",
    "    #   i_C(D,EPSILON){X}= {\n",
    "    #                      { +inf if ||X|| > EPSILON\n",
    "    #\n",
    "    #   X and Y are of size H x W x K x N, and EPSILON = exp(ALPHA)*V*STDN\n",
    "    #   is a scalar or a 1 x N vector, where V = sqrt(H*W*K-1).\n",
    "    #\n",
    "    #   Y = K*X where K = EPSILON / max(||X||,EPSILON);\n",
    "    # s.lefkimmiatis@skoltech.ru, 22/11/2016.\n",
    "    # pytorch implementation filippos.kokkinos@skoltech.ru 1/11/2017\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L2Proj, self).__init__()\n",
    "\n",
    "    def forward(self, x, stdn, alpha):\n",
    "        if x.is_cuda:\n",
    "            x_size = torch.cuda.FloatTensor(1).fill_(x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        else:\n",
    "            x_size = torch.Tensor([x.shape[1] * x.shape[2] * x.shape[3]])\n",
    "        numX = torch.sqrt(x_size-1)\n",
    "        if x.is_cuda:\n",
    "            epsilon = torch.cuda.FloatTensor(x.shape[0],1,1,1).fill_(1) * (torch.exp(alpha) * stdn * numX).view(-1,1,1,1)\n",
    "        else:\n",
    "            epsilon = torch.zeros(x.size(0),1,1,1).fill_(1) * (torch.exp(alpha) *  stdn * numX).view(-1,1,1,1)\n",
    "        x_resized = x.view(x.shape[0], -1)\n",
    "        x_norm = torch.norm(x_resized, 2, dim=1).view(x.size(0),1,1,1)\n",
    "        max_norm = torch.max(x_norm, epsilon)\n",
    "        result = x * (epsilon / max_norm)\n",
    "        return result\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=0, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, weightnorm=None, shortcut=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.shortcut = shortcut\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "\n",
    "        self.relu1 = nn.PReLU(num_parameters=planes,init=0.1)\n",
    "        self.relu2 = nn.PReLU(num_parameters=planes, init=0.1)\n",
    "        self.conv2 = conv3x3(inplanes, planes, stride)\n",
    "        if weightnorm:\n",
    "            self.conv1 = weight_norm(self.conv1)\n",
    "            self.conv2 = weight_norm(self.conv2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(x)\n",
    "        out = F.pad(out,(1,1,1,1),'reflect')\n",
    "        out = self.conv1(out)\n",
    "        out = out[:,:, :x.shape[2], :x.shape[3]]\n",
    "        out = self.relu2(out)\n",
    "        out = F.pad(out,(1,1,1,1),'reflect')\n",
    "        out = self.conv2(out)\n",
    "        out = out[:,:, :x.shape[2], :x.shape[3]]\n",
    "        if self.shortcut:\n",
    "            out = x + out\n",
    "        return out\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ResNet_Den(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_size, color=True, weightnorm=None):\n",
    "        self.inplanes = 64\n",
    "        block = BasicBlock\n",
    "        super(ResNet_Den, self).__init__()\n",
    "        if color:\n",
    "            in_channels = 3\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, stride=1, padding=0,\n",
    "                               bias=True)\n",
    "        if weightnorm:\n",
    "            self.conv1 = weight_norm(self.conv1)\n",
    "\n",
    "        # inntermediate layer has D-2 depth\n",
    "        self.layer1 = self._make_layer(block, 64, layer_size)\n",
    "        self.conv_out = nn.ConvTranspose2d(64, in_channels, kernel_size=5, stride=1, padding=2,\n",
    "                                  bias=True)\n",
    "        if weightnorm:\n",
    "            self.conv_out = weight_norm(self.conv_out)\n",
    "\n",
    "        self.l2proj = L2Proj()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                weights = np.sqrt(2/(9.*64))*np.random.standard_normal(m.weight.data.shape)\n",
    "                #weights = np.random.normal(size=m.weight.data.shape,\n",
    "                #                           scale=np.sqrt(1. / m.weight.data.shape[1]))\n",
    "                m.weight.data = torch.Tensor(weights)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        self.zeromean()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, weightnorm=True, shortcut=False))\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, weightnorm=True, shortcut=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def zeromean(self):\n",
    "        # Function zeromean subtracts the mean E(f) from filters f\n",
    "        # in order to create zero mean filters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data = m.weight.data - torch.mean(m.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.zeromean()\n",
    "        out = F.pad(x,(2,2,2,2),'reflect')\n",
    "        out = self.conv1(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.conv_out(out)\n",
    "        stdn = torch.FloatTensor(1).fill_(1)\n",
    "        sigma_min = 1\n",
    "        sigma_max = 2\n",
    "        alpha = nn.Parameter(torch.Tensor(1))\n",
    "        out = self.l2proj(out, stdn, alpha)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ResNet_Den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.denoise.dncnn import DnCNN\n",
    "dncnn = DnCNN(image_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   module name  input shape output shape  parameter quantity inference memory(MB)         MAdd duration percent\n",
      "0      dncnn.0    3 112  96   64 112  96                1792               2.62MB   37,158,912            1.98%\n",
      "1      dncnn.1   64 112  96   64 112  96                   0               2.62MB      688,128           37.08%\n",
      "2      dncnn.2   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.41%\n",
      "3      dncnn.3   64 112  96   64 112  96                 128               2.62MB    2,752,512            3.82%\n",
      "4      dncnn.4   64 112  96   64 112  96                   0               2.62MB      688,128            0.07%\n",
      "5      dncnn.5   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.83%\n",
      "6      dncnn.6   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.42%\n",
      "7      dncnn.7   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "8      dncnn.8   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.93%\n",
      "9      dncnn.9   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "10    dncnn.10   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "11    dncnn.11   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.46%\n",
      "12    dncnn.12   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "13    dncnn.13   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "14    dncnn.14   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.47%\n",
      "15    dncnn.15   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "16    dncnn.16   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "17    dncnn.17   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.46%\n",
      "18    dncnn.18   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "19    dncnn.19   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "20    dncnn.20   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.48%\n",
      "21    dncnn.21   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "22    dncnn.22   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "23    dncnn.23   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.47%\n",
      "24    dncnn.24   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "25    dncnn.25   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "26    dncnn.26   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.48%\n",
      "27    dncnn.27   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "28    dncnn.28   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "29    dncnn.29   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.48%\n",
      "30    dncnn.30   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.39%\n",
      "31    dncnn.31   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "32    dncnn.32   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.50%\n",
      "33    dncnn.33   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.38%\n",
      "34    dncnn.34   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "35    dncnn.35   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.76%\n",
      "36    dncnn.36   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.65%\n",
      "37    dncnn.37   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "38    dncnn.38   64 112  96   64 112  96               36864               2.62MB  792,035,328            3.17%\n",
      "39    dncnn.39   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.65%\n",
      "40    dncnn.40   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "41    dncnn.41   64 112  96   64 112  96               36864               2.62MB  792,035,328            3.16%\n",
      "42    dncnn.42   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.46%\n",
      "43    dncnn.43   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "44    dncnn.44   64 112  96   64 112  96               36864               2.62MB  792,035,328            2.54%\n",
      "45    dncnn.45   64 112  96   64 112  96                 128               2.62MB    2,752,512            0.45%\n",
      "46    dncnn.46   64 112  96   64 112  96                   0               2.62MB      688,128            0.06%\n",
      "47    dncnn.47   64 112  96    3 112  96                1728               0.12MB   37,126,656           10.53%\n",
      "===============================================================================================================\n",
      "total parameters quantity: 558,400\n",
      "total memory: 123.50MB\n",
      "total MAdd: 12,007,113,216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = dncnn\n",
    "input_size = (3, 112, 96)\n",
    "summary = model_summary(model, input_size, query_granularity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from networks.denoise.dncnn import DnCNN\n",
    "from utils import freeze_model\n",
    "dncnn = DnCNN(image_channels=3)\n",
    "\n",
    "model = dncnn.cuda()\n",
    "model = model.eval()\n",
    "model = model.train(False)\n",
    "freeze_model(model)\n",
    "input_size = (128, 3, 112, 96)\n",
    "input_tnsr = torch.randn(input_size).cuda()\n",
    "n = 10\n",
    "times = []\n",
    "for i in range(10):\n",
    "    s = time.time()\n",
    "    for i in range(n):\n",
    "        model(input_tnsr)\n",
    "    f = time.time()\n",
    "    times.append(f-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.69744062423706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(times)/10 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.478386402130127"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.339246084530411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(times)/10 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(im_shape, pattern='RGGB'):\n",
    "    if pattern == 'RGGB':\n",
    "        # pattern RGGB\n",
    "        r_mask = torch.zeros(im_shape)\n",
    "        r_mask[0::2, 0::2] = 1\n",
    "\n",
    "        g_mask = torch.zeros(im_shape)\n",
    "        g_mask[::2, 1::2] = 1\n",
    "        g_mask[1::2, ::2] = 1\n",
    "\n",
    "        b_mask = torch.zeros(im_shape)\n",
    "        b_mask[1::2, 1::2] = 1\n",
    "\n",
    "        mask = torch.zeros(im_shape + (3,))\n",
    "        mask[:, :, 0] = r_mask\n",
    "        mask[:, :, 1] = g_mask\n",
    "        mask[:, :, 2] = b_mask\n",
    "        \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.denoise.pydl import ResNet_Den\n",
    "from MMNet_TBPTT import *\n",
    "from problems import *\n",
    "from utils import freeze_model\n",
    "denoiser = ResNet_Den(5, weightnorm=True).cuda()\n",
    "denoiser = denoiser.eval()\n",
    "denoiser = denoiser.train(False)\n",
    "freeze_model(denoiser)\n",
    "\n",
    "max_iter = 5\n",
    "mmnet = MMNet(denoiser, max_iter=max_iter).cuda()\n",
    "mmnet = mmnet.eval()\n",
    "mmnet = mmnet.train(False)\n",
    "freeze_model(mmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mmnet\n",
    "\n",
    "input_size = (16, 3, 112, 96)\n",
    "input_tnsr = torch.randn(input_size).cuda()\n",
    "M = generate_mask((112, 96), pattern='RGGB').permute(2,0,1).unsqueeze(0).cuda()\n",
    "n = 10\n",
    "times = []\n",
    "for i in range(10):\n",
    "    s = time.time()\n",
    "    for i in range(n):\n",
    "        model.forward_all_iter(input_tnsr, M, init=False, noise_estimation=True)\n",
    "    f = time.time()\n",
    "    times.append(f-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.81122398376463"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(times)/10 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
