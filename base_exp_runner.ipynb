{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cur_logs_path = os.path.join(LOGS_DIR, args.name)\n",
    "    os.makedirs(cur_logs_path, exist_ok=True)\n",
    "    \n",
    "    cur_ckpt_path = os.path.join(CKPT_DIR, args.name)\n",
    "    os.makedirs(cur_ckpt_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "                'mmnet': {\n",
    "                    'model': mmnet,\n",
    "                    'load_ckpt': \n",
    "                },\n",
    "                'faceid': {\n",
    "                     'model': faceid,\n",
    "                     'load_ckpt': \"/home/safin/FaceReID/ckpt/joint_dnfr_16.04/faceid/weights_30\"\n",
    "                },\n",
    "                'argcmargin': {\n",
    "                     'model': arcmargin,\n",
    "                     'load_ckpt': \"/home/safin/FaceReID/ckpt/joint_dnfr_16.04/faceid/weights_30\"\n",
    "                }\n",
    "              }\n",
    "\n",
    "losses_dict = {\n",
    "                'L1': ,\n",
    "                'FaceID': \n",
    "            }\n",
    "\n",
    "optimizers_dict = {\n",
    "                    'general': \n",
    "                }\n",
    "\n",
    "log_dict = {\n",
    "            \n",
    "        }\n",
    "\n",
    "class BaseExpRunner():\n",
    "    def __init__(self, name, models_dict, schedulers_dict, optimizers_dict, losses_dict):\n",
    "        self.name = name\n",
    "        self.cur_logs_path = os.path.join(LOGS_DIR, name)\n",
    "        self.cur_ckpt_path = os.path.join(CKPT_DIR, name)\n",
    "        \n",
    "        self.models_dict = models_dict\n",
    "        self.schedulers_dict = schedulers_dict\n",
    "        self.optimizers_dict = optimizers_dict\n",
    "        self.losses_dict = losses_dict\n",
    "        self.logs_dict = {}\n",
    "        \n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.load_ckpts()\n",
    "        self.create_ckpt_dirs()\n",
    "        self.create_log_dirs()\n",
    "\n",
    "    def load_ckpts(self):\n",
    "        for model_dict in self.models_dict.keys():\n",
    "            model_ckpt_path = model_dict.get('load_ckpt')\n",
    "            if model_ckpt_path is not None:\n",
    "                model = model_dict['model']\n",
    "                model_dict['model'] = load_model(model, model_ckpt_path, self.multigpu_mode)\n",
    "\n",
    "    def create_ckpt_dirs(self):\n",
    "        for model_name in self.models_dict.keys():\n",
    "            model_ckpt_path = os.path.join(self.cur_ckpt_path, model_name)\n",
    "            self.models_dict[model_name]['ckpt_path'] = model_ckpt_path\n",
    "            os.makedirs(model_ckpt_path, exist_ok=True)\n",
    "    \n",
    "    def create_log_dirs(self):\n",
    "        os.makedirs(self.cur_logs_path, exist_ok=True)\n",
    "        for model_name in self.models_dict.keys():\n",
    "            self.logs_dict[model_name] = {\n",
    "                                            'path': os.path.join(self.cur_logs_path, \"train_loss_\" + self.name),\n",
    "                                            'data': []\n",
    "                                        }\n",
    "            \n",
    "    def save_ckpt(self, epoch):\n",
    "        for model_dict in self.models_dict.values():\n",
    "            save_model(model_dict['model'], os.path.join(model_dict['ckpt_path'], \"weights_%d\" % epoch), self.multigpu_mode)\n",
    "            \n",
    "    def save_logs(self):\n",
    "        for log in self.logs_dict.values():\n",
    "            log_path = log['path'] \n",
    "            log_data = log['data']\n",
    "            np.save(log_path, np.asarray(log_data))\n",
    "    \n",
    "    def global_forward(self, sample, batch_idx):\n",
    "        raise NotImplementedError(\"Each trainer should define global_forward() method\")\n",
    "        \n",
    "    def schedulers_step(self):\n",
    "        for scheduler in self.schedulers_dict.values():\n",
    "            scheduler.step()\n",
    "    \n",
    "    def train_epoch(self, train_dataloader):\n",
    "        for batch_idx, sample in enumerate(train_dataloader):\n",
    "            if stop_flag:\n",
    "                break\n",
    "            self.global_forward(sample, batch_idx)\n",
    "\n",
    "    def train(self, train_loader, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            if stop_flag:\n",
    "                break\n",
    "            self.schedulers_step()\n",
    "            self.train_epoch(train_dataloader)\n",
    "\n",
    "            self.save_ckpt(epoch)\n",
    "            self.save_logs()\n",
    "            self.cur_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utils import prepare_path\n",
    "from networks import networks_dict\n",
    "\n",
    "logs_dir = \"logs\"\n",
    "ckpt_dir = \"ckpt\"\n",
    "\n",
    "models = {\n",
    "    \"faceid\": \"sphereface\",\n",
    "    \"denoiser\": \"udnet\"\n",
    "}\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, models, exp_name, losses, dataloader):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models (dict): Dictionary with a models to be trained.\n",
    "            param2 (str): The second parameter.\n",
    "        \"\"\"\n",
    "        self.model_is_initialized = False\n",
    "        self.models = models\n",
    "        slef.models_dict = {}\n",
    "        self.models_ckpt_path = {}\n",
    "        self.ckpt_folder_path = os.path.join(ckpt_dir, exp_name)\n",
    "\n",
    "    def init_models(self, cuda=True, last_ckpt=None):\n",
    "        for model in self.models.items():\n",
    "            model_name, model_type = model\n",
    "            self.models_dict[model_name] = networks_dict[model_type]()\n",
    "        \n",
    "        for opt in self.opts.items():\n",
    "            opt_name, opt_params = opt\n",
    "            opt_params = [self.models_dict[model_name].parameters() for model_name in opt_params]\n",
    "            \n",
    "            if len(opt_params) > 1:\n",
    "                opt_params = itertools.chain(*opt_params)\n",
    "            else:\n",
    "                opt_params = opt_params[0]\n",
    "            \n",
    "            self.opt_dict[opt_name] = torch.optim.Adam(opt_params, lr=1e-4)\n",
    "        \n",
    "        if last_ckpt is not None:        \n",
    "            for model_obj in self.models.items():\n",
    "                model_name, model = model_obj\n",
    "                ckpt_path = os.path.join(ckpt_folder_path, model_name)\n",
    "                load_model(model, optimizer, ckpt_path)\n",
    "            if last_ckpt is not None:\n",
    "                state_dict = torch.load(last_ckpt)\n",
    "                self.model.load_state_dict(state_dict)\n",
    "        \n",
    "        if cuda:\n",
    "            for k in self.model_dict.keys():\n",
    "                self.model_dict[k] = self.model_dict[k].cuda()\n",
    "        \n",
    "            \n",
    "        self.model = self.model.cuda()\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.,8.])).cuda()\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.model_is_initialized = True\n",
    "\n",
    "    def run_experiments(self, exp_name, n_epochs, batch_size=2):\n",
    "        assert self.model_is_initialized, \"Model had not been initialized! Use init_model()!\"\n",
    "        \n",
    "        cur_logs_path = os.path.join(logs_dir, args.name)\n",
    "        os.makedirs(cur_logs_path, exist_ok=True)\n",
    "        \n",
    "#         self.writer = SummaryWriter(cur_logs_path)\n",
    "\n",
    "        cur_ckpt_path = os.path.join(ckpt_dir, args.name)\n",
    "        os.makedirs(cur_ckpt_path, exist_ok=True)\n",
    "        \n",
    "        for k in self.models_dict.keys():\n",
    "            model_ckpt_path = os.path.join(cur_ckpt_path, k)\n",
    "            os.makedirs(model_ckpt_path, exist_ok=True)\n",
    "            self.models_ckpt_path[k] = model_ckpt_path\n",
    "\n",
    "\n",
    "        train_data = Brains(crop_size=(64,64), proper_crop_proba=0.5)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=20)\n",
    "        \n",
    "        val_data = Brains(folder='../scanmasks-val/', crop=False, proper_crop_proba=-1)\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=True, drop_last=False, num_workers=0)\n",
    "        \n",
    "        global_train_i = 0\n",
    "        for i in range(n_epochs):\n",
    "            print(\"Training: epoch %d\" % i)\n",
    "            train_loss, global_train_i = self.train_epoch(i, train_dataloader, global_train_i)\n",
    "            val_loss = self.validate(i, val_dataloader)\n",
    "            \n",
    "            if i%10==0:\n",
    "                torch.save(self.model.state_dict(), ckpt_folder_path + \"model_%d_epochs\" % i)\n",
    "            \n",
    "            \n",
    "    def train_epoch(self, n_epoch, dataloader, i):\n",
    "        self.model.train(True)\n",
    "    \n",
    "        epoch_ce_loss = []\n",
    "        epoch_dice = []\n",
    "        \n",
    "        for X_batch, masks_batch in dataloader:\n",
    "        #         X_batch = X_batch[0]\n",
    "        #         X_batch = X_batch.float()\n",
    "            probs = self.model(X_batch.cuda())\n",
    "            _, preds = probs.max(dim=1, keepdim=True)\n",
    "\n",
    "        #         print(preds.shape, masks_batch.shape)\n",
    "            loss = self.criterion(probs, masks_batch.squeeze(1).cuda())\n",
    "            self.writer.add_scalar('Cross entropy loss per iter', loss.item(), i)\n",
    "            dice = dice_score(preds.float(), masks_batch.cuda().float()).mean()\n",
    "            self.writer.add_scalar('Dice score', dice.item(), i)\n",
    "        #         print(X_batch.shape)\n",
    "        #         print(torchvision.utils.make_grid(X_batch).shape)\n",
    "            bool_mask, idx = masks_batch.reshape((*masks_batch.shape[:3],-1)).max(dim=-1)\n",
    "            if torch.any(bool_mask.reshape((1,-1)).byte()==1):\n",
    "                self.writer.add_image('Input image vs mask vs pred', torch.cat((X_batch[bool_mask.byte()][0], masks_batch[bool_mask.byte()][0].float(), preds[bool_mask.byte()][0].cpu().float()), dim=1), i)\n",
    "\n",
    "        #         score = dice_score(F.threshold(preds, 0.5, 1), masks_batch)\n",
    "\n",
    "            # train on batch\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            i += 1\n",
    "            \n",
    "            epoch_ce_loss.append(loss.cpu().data.numpy())\n",
    "            epoch_dice.append(dice.cpu().data.numpy())\n",
    "            \n",
    "        total_ce_loss = np.mean(epoch_ce_loss)\n",
    "        self.writer.add_scalar('Train cross entropy loss per epoch', total_ce_loss, n_epoch)\n",
    "        self.writer.add_scalar('Train dice per epoch', np.mean(epoch_dice), n_epoch)\n",
    "        return total_ce_loss, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ExpRunner():\n",
    "    def train_epoch(self):\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            train_loss_arr = []\n",
    "            denoiser_loss_arr = []\n",
    "            faceid_loss_arr = []\n",
    "\n",
    "            total_loss = 0\n",
    "    #         faceid_w = 1\n",
    "    #         denoise_w = 0\n",
    "    #         if epoch >= 1:\n",
    "    #             faceid_w = 1\n",
    "    #         else:\n",
    "    #             faceid_w = 0\n",
    "    #         \n",
    "    #         if epoch in [0,10,15,18]:\n",
    "    #             if epoch!=0: lr *= 0.1 #lr *= 0.9\n",
    "    #             optimizer = optim.SGD(itertools.chain(faceid.parameters(), ArcMargin.parameters()), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            scheduler.step()\n",
    "            total_loss = train_epoch(dataloader_train, optimizer, total, correct, total_loss, train_loss_arr, total_denoiser_loss_arr, total_faceid_loss_arr)\n",
    "\n",
    "            save_model(mmnet, os.path.join(denoiser_ckpt_path, \"weights_%d\" % epoch), multigpu_mode)\n",
    "            save_model(faceid, os.path.join(faceid_ckpt_path, \"weights_%d\" % epoch), multigpu_mode)\n",
    "            save_model(ArcMargin, os.path.join(arcmargin_ckpt_path, \"weights_%d\" % epoch), multigpu_mode)\n",
    "\n",
    "            total_train_loss_arr.append(np.mean(train_loss_arr))\n",
    "            np.save(os.path.join(cur_logs_path,\"train_loss_\" + args.name), np.asarray(total_train_loss_arr))\n",
    "            total_denoiser_loss_arr.append(np.mean(denoiser_loss_arr))\n",
    "            np.save(os.path.join(cur_logs_path,\"denoiser_loss_\" + args.name), np.asarray(total_denoiser_loss_arr))\n",
    "            total_faceid_loss_arr.append(np.mean(faceid_loss_arr))\n",
    "            np.save(os.path.join(cur_logs_path,\"faceid_loss_\" + args.name), np.asarray(total_faceid_loss_arr))\n",
    "\n",
    "    #         total_train_acc_arr.append(100. * correct/total)\n",
    "    #         np.save(os.path.join(cur_logs_path,\"train_faceid_acc_\" + args.name), np.asarray(total_train_acc_arr))\n",
    "\n",
    "            grads = []\n",
    "            for idx, p in enumerate(list(filter(lambda p: p.grad is not None, faceid.parameters()))):\n",
    "                grads.append([idx, p.grad.data.norm(2).item()])\n",
    "            np.save(os.path.join(cur_logs_path,\"train_grads_\" + args.name  + \"_%d\" % epoch), np.asarray(grads))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    #         total = 0\n",
    "    #         correct = 0\n",
    "    #         train_loss_arr = []\n",
    "    #         total_loss = 0\n",
    "    #         train_epoch(dataloader_val, None, total, correct, total_loss, train_loss_arr)\n",
    "    #         print(\"\\n\")\n",
    "    #         torch.save(denoiser.state_dict(), ckpt_path + \"denoiser_\" + args.name + \"_%d\" % epoch)\n",
    "    #         np.save(\"train_loss_\" + args.name + \"_%d\" % epoch, np.asarray(train_loss_arr))\n",
    "\n",
    "\n",
    "            if stop_flag:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
